ğŸ“Š Reproducible Batch Data Pipeline with Spark, Airflow & PostgreSQL

Project Overview : 

This project implements an end-to-end batch-processing data architecture for large-scale intraday stock market data. 

The system ingests 1-minute OHLC data, performs distributed rolling analytics using Apache Spark, and generates ML-ready crash detection features.

The entire pipeline is containerized using Docker and orchestrated via Apache Airflow, ensuring reproducibility, reliability, and governance-aware execution.

ğŸ— Architecture

The system follows a microservices-based batch architecture:

CSV Dataset

    â†“
    
Ingestion Service (Python + COPY)

    â†“
    
PostgreSQL (Raw Storage)

    â†“
    
Spark Batch Analytics

    â†“
    
PostgreSQL (Feature Store)

    â†“
    
Verification & Metadata Layer


Core Components

Layer	                                            Technology	                             Purpose
Orchestration	                                    Apache Airflow	                         DAG scheduling & monitoring
Ingestion		                         	            Python + psycopg2		                     Deterministic CSV ingestion
Raw Storage		                         	          PostgreSQL		                           Persistent raw data store
Processing		                                    Apache Spark		                         Distributed rolling analytics
Feature Store		                                  PostgreSQL		                           ML-ready analytics table
Runtime		                         	              Docker Compose		                       Infrastructure as Code



âš™ Key Engineering Features

ğŸ”¹ Deterministic Ingestion

â€¢	SHA256 dataset fingerprinting

â€¢	Guardrails preventing duplicate ingestion

â€¢	Transaction-safe bulk loading via PostgreSQL COPY

â€¢	Row-count read-back validation

â€¢	Idempotent execution

ğŸ”¹ Distributed Rolling Analytics

â€¢	Window-based momentum and volatility metrics

â€¢	VWAP and drawdown computation

â€¢	Composite crash index construction

â€¢	Automatic threshold selection (F1-score optimized)

ğŸ”¹ Data Integrity & Governance

â€¢	Duplicate timestamp detection

â€¢	Time-gap validation (1-minute consistency)

â€¢	Null and anomaly checks

â€¢	Deterministic MD5 checksum on analytics output

â€¢	SQL-level verification layer

ğŸ”¹ Drift Detection

â€¢	Statistical Z-score-based drift monitoring

â€¢	Historical run comparison

â€¢	Metadata persistence per pipeline execution

ğŸ”¹ Reliability & Performance

â€¢	Exponential retry mechanisms

â€¢	Transaction rollback protection

â€¢	PostgreSQL WAL tuning for bulk writes

â€¢	Spark partitioned JDBC writes

â€¢	Controlled container memory limits

ğŸ“‚ Project Structure

Airflow_batch-stock-market-pipeline/

â”‚

â”œâ”€â”€ airflow/

â”‚   â”œâ”€â”€ dags/

â”‚   â”‚   â””â”€â”€ stock_batch_pipeline_dag.py

â”‚   â””â”€â”€ logs/

â”‚

â”œâ”€â”€ ingestion/

â”‚   â”œâ”€â”€ Dockerfile

â”‚   â”œâ”€â”€ ingest.py

â”‚   â””â”€â”€ requirements.txt

â”‚

â”œâ”€â”€ data/

â”‚   â”œâ”€â”€ 1_min_SPY_2008-2021.csv

â”‚

â”œâ”€â”€ spark/

â”‚   â”œâ”€â”€ Dockerfile

â”‚   â”œâ”€â”€ entrypoint.sh

â”‚   â””â”€â”€ batch_job.py

â”‚

â”œâ”€â”€ db/

â”‚   â”œâ”€â”€ 01_create_stockdb.sql

â”‚   â””â”€â”€ 02_stock_schema.sql

â”‚

â”œâ”€â”€ docker-compose.yml

â”œâ”€â”€ .env

â””â”€â”€ .dockerignore

ğŸš€ How to Run

1ï¸âƒ£ Prerequisites

Docker

Docker Compose

Minimum 8GB RAM recommended


4ï¸âƒ£ Trigger the Pipeline
DAG name:
stock_batch_pipeline
Pipeline stages:
1.	wait_for_postgres
2.	run_ingestion
3.	run_spark
4.	verify_analytics
5.	finalize_pipeline
Successful execution confirms end-to-end consistency.
________________________________________
ğŸ—„ Database Tables
Raw Layer
â€¢	raw_stock_prices
Feature Layer
â€¢	stock_intraday_features
Metadata Tables
â€¢	ingestion_metadata
â€¢	pipeline_run_metadata
________________________________________
ğŸ” Reproducibility
This system is fully reproducible through:
â€¢	Docker Compose
â€¢	Environment-based configuration
â€¢	SQL initialization scripts
â€¢	Version-controlled infrastructure
â€¢	Deterministic dataset fingerprinting
Running the pipeline on another machine produces identical analytical results.
________________________________________
ğŸ“ˆ Performance Optimizations
â€¢	WAL & checkpoint tuning in PostgreSQL
â€¢	Chunked ingestion (200k rows per batch)
â€¢	Spark shuffle partition tuning
â€¢	JDBC batch size optimization
â€¢	Memory allocation controls
________________________________________
âš  Limitations
â€¢	Single-node Spark execution
â€¢	Manual DAG trigger (batch-only mode)
â€¢	No cloud deployment layer
â€¢	No horizontal database scaling
________________________________________
ğŸ”® Future Enhancements
â€¢	Real-time streaming pipeline (Kafka + Spark Structured Streaming)
â€¢	Kubernetes deployment
â€¢	CI/CD integration
â€¢	Automated data quality dashboards
â€¢	Cloud-native scaling

# ğŸ“Š Reproducible Batch Data Pipeline with Spark, Airflow & PostgreSQL

## ğŸ“Œ Project Overview

This project implements an end-to-end **batch-processing data architecture** for large-scale intraday stock market data. The system ingests 1-minute OHLC data, performs distributed rolling analytics using Apache Spark, and generates ML-ready crash detection features.

The architecture is fully containerized using Docker, orchestrated via Apache Airflow, and designed with deterministic ingestion, validation, metadata lineage tracking, and statistical drift detection.

The system demonstrates production-style reliability, reproducibility, and governance-aware batch execution.

---

# ğŸ— Architecture

The pipeline follows a microservices-based batch architecture:

CSV Dataset
â†“
Ingestion Service (Python + COPY)
â†“
PostgreSQL (Raw Storage)
â†“
Spark Batch Analytics
â†“
PostgreSQL (Feature Store)
â†“
Verification & Metadata Layer


## Core Components

| Layer | Technology | Responsibility |
|--------|------------|----------------|
| Orchestration | Apache Airflow | DAG scheduling & monitoring |
| Ingestion | Python + psycopg2 | Deterministic CSV ingestion |
| Raw Storage | PostgreSQL | Persistent raw data store |
| Processing | Apache Spark | Distributed rolling analytics |
| Feature Store | PostgreSQL | ML-ready analytics features |
| Runtime | Docker Compose | Infrastructure as Code |

---

# âš™ Key Engineering Features

## ğŸ”¹ Deterministic & Idempotent Ingestion
- SHA256 dataset fingerprinting
- Guardrails preventing duplicate ingestion
- Transaction-safe bulk loading (PostgreSQL COPY)
- Row-count read-back validation
- Controlled table truncation

## ğŸ”¹ Distributed Rolling Analytics
- Intraday window functions (5â€“60 minute rolling metrics)
- Volatility normalization (Z-score)
- VWAP and drawdown computation
- Composite crash index construction
- Automatic F1-score optimized threshold selection

## ğŸ”¹ Data Integrity & Governance
- Duplicate timestamp detection
- Time-gap validation (1-minute continuity)
- Logical consistency checks
- Deterministic MD5 checksum of feature table
- SQL-based verification layer

## ğŸ”¹ Statistical Drift Detection
- Z-score-based drift monitoring
- Historical run comparison
- Metadata persistence per pipeline execution

## ğŸ”¹ Reliability & Performance Engineering
- Exponential retry mechanisms
- Transaction rollback protection
- PostgreSQL WAL tuning
- Partitioned JDBC writes
- Container memory isolation

---

# ğŸ“‚ Project Structure

Airflow_batch-stock-market-pipeline/
â”‚
â”œâ”€â”€ airflow/
â”‚ â”œâ”€â”€ dags/
â”‚ â”‚ â””â”€â”€ stock_batch_pipeline_dag.py
â”‚ â””â”€â”€ logs/
â”‚
â”œâ”€â”€ ingestion/
â”‚ â”œâ”€â”€ Dockerfile
â”‚ â”œâ”€â”€ ingest.py
â”‚ â””â”€â”€ requirements.txt
â”‚
â”œâ”€â”€ spark/
â”‚ â”œâ”€â”€ Dockerfile
â”‚ â”œâ”€â”€ entrypoint.sh
â”‚ â””â”€â”€ batch_job.py
â”‚
â”œâ”€â”€ db/
â”‚ â”œâ”€â”€ 01_create_stockdb.sql
â”‚ â””â”€â”€ 02_stock_schema.sql
â”‚
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ .env
â””â”€â”€ .dockerignore


---

# ğŸ”§ Environment Configuration

Create a `.env` file in the project root:

Airflow Metadata Database

AIRFLOW_DB=airflowdb
Business PostgreSQL Database

STOCK_DB=stockdb
POSTGRES_USER=stockuser
POSTGRES_PASSWORD=stockpass
POSTGRES_HOST=postgres
POSTGRES_PORT=5432
Airflow Postgres Connection

AIRFLOW_CONN_POSTGRES_DEFAULT=postgres://stockuser:stockpass@postgres:5432/stockdb
Dataset Path (mounted volume)

CSV_PATH=/data/1_min_SPY_2008-2021.csv
Spark Runtime Configuration

SPARK_CONF=--conf spark.executor.memory=4g
--conf spark.driver.memory=4g
--conf spark.sql.shuffle.partitions=8
--conf spark.memory.fraction=0.7
--conf spark.sql.codegen.aggregate.map.twolevel.enabled=false


> All configuration is environment-driven. No credentials are hardcoded inside application source code.

---

# ğŸ’» System Requirements

- Docker
- Docker Compose
- Minimum 8GB RAM recommended
- Port 8080 available

---

# ğŸš€ How to Run

## Step 1 â€“ Clean Start (Recommended First Run)

docker compose down -v

Step 2 â€“ Build & Start Infrastructure

docker compose up --build

This initializes:

    PostgreSQL

    Dataset volume seeding

    Airflow metadata DB

    Airflow admin user

    Airflow webserver & scheduler

ğŸŒ Access Airflow

Open:

http://localhost:8080

Login:

Username: admin
Password: admin

â–¶ Trigger the Pipeline

DAG name:

stock_batch_pipeline

Execution stages:

    wait_for_postgres

    run_ingestion

    run_spark

    verify_analytics

    finalize_pipeline

âœ… Verification Checklist

A successful run should show:
Ingestion

Verification status : SUCCESS

Spark

Spark job completed successfully

Database Verification

DB VERIFICATION COMPLETED SUCCESSFULLY

Finalization

PIPELINE COMPLETED SUCCESSFULLY

ğŸ—„ Database Tables
Raw Layer

    raw_stock_prices

Feature Layer

    stock_intraday_features

Metadata Tables

    ingestion_metadata

    pipeline_run_metadata

ğŸ”„ Re-running the Pipeline

The pipeline supports idempotent execution:

    Duplicate dataset ingestion prevented

    Spark overwrites feature table safely

    Metadata updated per run

    Drift comparison executed automatically

ğŸ“ˆ Performance Optimizations

    WAL & checkpoint tuning (PostgreSQL)

    Chunked ingestion (200k rows per batch)

    Spark shuffle partition tuning

    Partitioned JDBC batch writes

    Controlled container memory allocation

âš  Limitations

    Single-node Spark execution

    Manual DAG trigger (batch mode only)

    No horizontal database scaling

    Local deployment only

ğŸ”® Future Enhancements

    Real-time streaming extension (Kafka + Spark Structured Streaming)

    Kubernetes deployment

    CI/CD integration

    Automated data quality dashboards

    Cloud-native scaling

ğŸ‘¨â€ğŸ’» Author
