# ğŸ“Š Reproducible Stock Market Batch Data Pipeline with Spark, Airflow & PostgreSQL

## ğŸ“Œ Project Overview

This project implements an end-to-end **batch-processing data architecture** for large-scale intraday stock market data. The system ingests 1-minute OHLC data(**more than 1,000,000 rows**), performs distributed rolling analytics using Apache Spark, and generates ML-ready crash detection features.

The architecture is fully containerized using Docker, orchestrated via Apache Airflow, and designed with deterministic ingestion, validation, metadata lineage tracking, and statistical drift detection.

The system demonstrates production-style reliability, reproducibility, and governance-aware batch execution.

---

# ğŸ— Architecture

The pipeline follows a microservices-based batch architecture:

        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚        CSV Dataset           â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚
                    â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚  Ingestion Service           â”‚
        â”‚  (Python + COPY)             â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚
                    â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚  PostgreSQL                  â”‚
        â”‚  (Raw Storage)               â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚
                    â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚  Spark Batch Analytics       â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚
                    â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚  PostgreSQL                  â”‚
        â”‚  (Feature Store)             â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚
                    â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚  Verification &              â”‚
        â”‚  Metadata Layer              â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

## Core Components

| Layer   | Technology | Responsibility   |
|  --------  |  ------------  |  ----------------  |
|   Orchestration   |   Apache Airflow   |   DAG scheduling & monitoring   |
|   Ingestion   |   Python + psycopg2   |   Deterministic CSV ingestion   |
|   Raw Storage   |   PostgreSQL   |   Persistent raw data store   |
|   Processing   |   Apache Spark   |   Distributed rolling analytics   |
|   Feature Store   |   PostgreSQL   |   ML-ready analytics features   |
|   Runtime   |   Docker Compose   |   Infrastructure as Code   |

---
# ğŸ“‚ Project Structure

    Airflow_batch-stock-market-pipeline/
    â”‚
    â”œâ”€â”€ airflow/                             # Airflow orchestration layer
    â”‚ â”œâ”€â”€ dags/
    â”‚ â”‚ â””â”€â”€ stock_batch_pipeline_dag.py      # Main DAG defining pipeline workflow
    â”‚ â””â”€â”€ logs/                              # Airflow task execution logs
    â”‚
    â”œâ”€â”€ ingestion/                           # Data ingestion microservice
    â”‚ â”œâ”€â”€ Dockerfile                         # Container definition for ingestion service
    â”‚ â”œâ”€â”€ ingest.py                          # CSV validation & bulk COPY ingestion logic
    â”‚ â””â”€â”€ requirements.txt                   # Python dependencies for ingestion
    â”‚
    â”œâ”€â”€ spark/                               # Spark batch analytics microservice
    â”‚ â”œâ”€â”€ Dockerfile                         # Spark runtime container definition
    â”‚ â”œâ”€â”€ entrypoint.sh                      # Spark submit entry script
    â”‚ â””â”€â”€ batch_job.py                       # Distributed rolling analytics & crash detection
    â”‚
    â”œâ”€â”€ db/                                  # Database initialization scripts
    â”‚ â”œâ”€â”€ 01_create_stockdb.sql              # Creates business database
    â”‚ â””â”€â”€ 02_stock_schema.sql                # Creates raw, feature & metadata tables
    â”‚
    â”œâ”€â”€ docker-compose.yml                   # Infrastructure orchestration (IaC)
    â”œâ”€â”€ .env                                 # Environment variables (credentials & configs)
    â”œâ”€â”€ .dockerignore                        # Files excluded from Docker build context
    â””â”€â”€ README.md                            # Project documentation

---

# ğŸ”§ Environment Configuration

Create a `.env` file in the project root:

```

# Airflow Metadata Database

AIRFLOW_DB=airflowdb

# Business PostgreSQL Database

STOCK_DB=stockdb
POSTGRES_USER=Your_Postgres_User
POSTGRES_PASSWORD=Your_Postgres_Password
POSTGRES_HOST=postgres
POSTGRES_PORT=5432
AIRFLOW_USER=Your_Airflow_User
AIRFLOW_PASSWORD=Your_Airflow_Password

# Airflow Postgres Connection

AIRFLOW_CONN_POSTGRES_DEFAULT=postgres://${POSTGRES_USER}:${POSTGRES_PASSWORD}@postgres:5432/${STOCK_DB}

# Dataset Path (mounted volume)

CSV_PATH=/data/1_min_SPY_2008-2021.csv

# Spark Runtime Configuration

SPARK_CONF=--conf spark.executor.memory=4g 
--conf spark.driver.memory=4g 
--conf spark.sql.shuffle.partitions=8 
--conf spark.memory.fraction=0.7 
--conf spark.sql.codegen.aggregate.map.twolevel.enabled=false

````

> All configuration is environment-driven. No credentials are hardcoded inside application source code.

---

# ğŸ’» System Requirements

- Docker
- Docker Compose
- Minimum 8GB RAM recommended
- Port 8080 available

---

# ğŸš€ How to Run

## Step 1 â€“ Clean Start - Recommended First Run

    docker compose down -v

## Step 2 â€“ Build & Start Infrastructure

    docker compose up --build

### This initializes:

* PostgreSQL
* Dataset volume seeding
* Airflow metadata DB
* Airflow admin user
* Airflow webserver & scheduler

---

# ğŸŒ Access Airflow

Open:

```
http://localhost:8080
```

Login:

```
Username: Your_Airflow_User
Password: Your_Airflow_Password
```

---

# â–¶ Trigger the Pipeline

DAG name:

```
stock_batch_pipeline
```

Execution stages:

1. wait_for_postgres
2. run_ingestion
3. run_spark
4. verify_analytics
5. finalize_pipeline

---

# âœ… Verification Checklist

A successful run should show:

## Ingestion

```
Verification status : SUCCESS
```

## Spark

```
Spark job completed successfully
```

## Database Verification

```
DB VERIFICATION COMPLETED SUCCESSFULLY
```

## Finalization

```
PIPELINE COMPLETED SUCCESSFULLY
```
---

# ğŸ—„ Database Tables

## Raw Layer

* raw_stock_prices

## Feature Layer

* stock_intraday_features

## Metadata Tables

* ingestion_metadata
* pipeline_run_metadata

---

# âš™ Key Engineering Features

## ğŸ”¹ Deterministic & Idempotent Ingestion
- SHA256 dataset fingerprinting
- Guardrails preventing duplicate ingestion
- Transaction-safe bulk loading (PostgreSQL COPY)
- Row-count read-back validation
- Controlled table truncation

## ğŸ”¹ Distributed Rolling Analytics
- Intraday window functions (5â€“60 minute rolling metrics)
- Volatility normalization (Z-score)
- VWAP and drawdown computation
- Composite crash index construction
- Automatic F1-score optimized threshold selection

## ğŸ”¹ Data Integrity & Governance
- Duplicate timestamp detection
- Time-gap validation (1-minute continuity)
- Logical consistency checks
- Deterministic MD5 checksum of feature table
- SQL-based verification layer

## ğŸ”¹ Statistical Drift Detection
- Z-score-based drift monitoring
- Historical run comparison
- Metadata persistence per pipeline execution

## ğŸ”¹ Reliability & Performance Engineering
- Exponential retry mechanisms
- Transaction rollback protection
- PostgreSQL WAL tuning
- Partitioned JDBC writes
- Container memory isolation
---

# ğŸ”„ Re-running the Pipeline

The pipeline supports idempotent execution:

* Duplicate dataset ingestion prevented
* Spark overwrites feature table safely
* Metadata updated per run
* Drift comparison executed automatically

---

# ğŸ“ˆ Performance Optimizations

* WAL & checkpoint tuning (PostgreSQL)
* Chunked ingestion (200k rows per batch)
* Spark shuffle partition tuning
* Partitioned JDBC batch writes
* Controlled container memory allocation

---

# âš  Limitations

* Single-node Spark execution
* Manual DAG trigger (batch mode only)
* No horizontal database scaling
* Local deployment only

---

# ğŸ”® Future Enhancements

* Real-time streaming extension (Kafka + Spark Structured Streaming)
* Kubernetes deployment
* CI/CD integration
* Automated data quality dashboards
* Cloud-native scaling

---

# ğŸ‘¨â€ğŸ’» Author
