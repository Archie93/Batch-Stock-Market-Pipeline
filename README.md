ğŸ“Š Reproducible Batch Data Pipeline with Spark, Airflow & PostgreSQL

Project Overview : 

This project implements an end-to-end batch-processing data architecture for large-scale intraday stock market data. 

The system ingests 1-minute OHLC data, performs distributed rolling analytics using Apache Spark, and generates ML-ready crash detection features.

The entire pipeline is containerized using Docker and orchestrated via Apache Airflow, ensuring reproducibility, reliability, and governance-aware execution.

ğŸ— Architecture

The system follows a microservices-based batch architecture:

CSV Dataset

    â†“
    
Ingestion Service (Python + COPY)

    â†“
    
PostgreSQL (Raw Storage)

    â†“
    
Spark Batch Analytics

    â†“
    
PostgreSQL (Feature Store)

    â†“
    
Verification & Metadata Layer


Core Components

Layer	                                            Technology	                             Purpose
Orchestration	                                    Apache Airflow	                         DAG scheduling & monitoring
Ingestion		                         	            Python + psycopg2		                     Deterministic CSV ingestion
Raw Storage		                         	          PostgreSQL		                           Persistent raw data store
Processing		                                    Apache Spark		                         Distributed rolling analytics
Feature Store		                                  PostgreSQL		                           ML-ready analytics table
Runtime		                         	              Docker Compose		                       Infrastructure as Code



âš™ Key Engineering Features

ğŸ”¹ Deterministic Ingestion

â€¢	SHA256 dataset fingerprinting

â€¢	Guardrails preventing duplicate ingestion

â€¢	Transaction-safe bulk loading via PostgreSQL COPY

â€¢	Row-count read-back validation

â€¢	Idempotent execution

ğŸ”¹ Distributed Rolling Analytics

â€¢	Window-based momentum and volatility metrics

â€¢	VWAP and drawdown computation

â€¢	Composite crash index construction

â€¢	Automatic threshold selection (F1-score optimized)

ğŸ”¹ Data Integrity & Governance

â€¢	Duplicate timestamp detection

â€¢	Time-gap validation (1-minute consistency)

â€¢	Null and anomaly checks

â€¢	Deterministic MD5 checksum on analytics output

â€¢	SQL-level verification layer

ğŸ”¹ Drift Detection

â€¢	Statistical Z-score-based drift monitoring

â€¢	Historical run comparison

â€¢	Metadata persistence per pipeline execution

ğŸ”¹ Reliability & Performance

â€¢	Exponential retry mechanisms

â€¢	Transaction rollback protection

â€¢	PostgreSQL WAL tuning for bulk writes

â€¢	Spark partitioned JDBC writes

â€¢	Controlled container memory limits

ğŸ“‚ Project Structure

Airflow_batch-stock-market-pipeline/

â”‚

â”œâ”€â”€ airflow/

â”‚   â”œâ”€â”€ dags/

â”‚   â”‚   â””â”€â”€ stock_batch_pipeline_dag.py

â”‚   â””â”€â”€ logs/

â”‚

â”œâ”€â”€ ingestion/

â”‚   â”œâ”€â”€ Dockerfile

â”‚   â”œâ”€â”€ ingest.py

â”‚   â””â”€â”€ requirements.txt

â”‚

â”œâ”€â”€ data/

â”‚   â”œâ”€â”€ 1_min_SPY_2008-2021.csv

â”‚

â”œâ”€â”€ spark/

â”‚   â”œâ”€â”€ Dockerfile

â”‚   â”œâ”€â”€ entrypoint.sh

â”‚   â””â”€â”€ batch_job.py

â”‚

â”œâ”€â”€ db/

â”‚   â”œâ”€â”€ 01_create_stockdb.sql

â”‚   â””â”€â”€ 02_stock_schema.sql

â”‚

â”œâ”€â”€ docker-compose.yml

â”œâ”€â”€ .env

â””â”€â”€ .dockerignore

ğŸš€ How to Run

1ï¸âƒ£ Prerequisites

Docker

Docker Compose

Minimum 8GB RAM recommended


4ï¸âƒ£ Trigger the Pipeline
DAG name:
stock_batch_pipeline
Pipeline stages:
1.	wait_for_postgres
2.	run_ingestion
3.	run_spark
4.	verify_analytics
5.	finalize_pipeline
Successful execution confirms end-to-end consistency.
________________________________________
ğŸ—„ Database Tables
Raw Layer
â€¢	raw_stock_prices
Feature Layer
â€¢	stock_intraday_features
Metadata Tables
â€¢	ingestion_metadata
â€¢	pipeline_run_metadata
________________________________________
ğŸ” Reproducibility
This system is fully reproducible through:
â€¢	Docker Compose
â€¢	Environment-based configuration
â€¢	SQL initialization scripts
â€¢	Version-controlled infrastructure
â€¢	Deterministic dataset fingerprinting
Running the pipeline on another machine produces identical analytical results.
________________________________________
ğŸ“ˆ Performance Optimizations
â€¢	WAL & checkpoint tuning in PostgreSQL
â€¢	Chunked ingestion (200k rows per batch)
â€¢	Spark shuffle partition tuning
â€¢	JDBC batch size optimization
â€¢	Memory allocation controls
________________________________________
âš  Limitations
â€¢	Single-node Spark execution
â€¢	Manual DAG trigger (batch-only mode)
â€¢	No cloud deployment layer
â€¢	No horizontal database scaling
________________________________________
ğŸ”® Future Enhancements
â€¢	Real-time streaming pipeline (Kafka + Spark Structured Streaming)
â€¢	Kubernetes deployment
â€¢	CI/CD integration
â€¢	Automated data quality dashboards
â€¢	Cloud-native scaling
